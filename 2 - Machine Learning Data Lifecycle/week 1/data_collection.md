### A data pipeline is a series of data processing steps such as:
* Data collection.
* Data ingestion is the process of absorbing data from different sources and transferring it to a target site where it can be deposited and analyzed.
* Data preparation consists of data formatting, engineering and feature extraction.

### What do you apply to maximize predictive signals in your data?
* Feature engineering. It is the process of using domain knowledge to extract features with high levels of predictive signal from raw data.

### Your training data should reflect the diversity and cultural context of the people who will use it. What can be done to mitigate inherent biases in a given data set?
* 
Collect data from equal proportions from different user groups. Balanced sampling from different user groups helps avoid inherent biases.

### More often than not,  ML systems can fail the users it serves. In this context, what is representational harm?
* The amplification or negative reflection of certain groups stereotypes.

### Accurate labels are necessary to properly train supervised models. Many times, human subjects known as raters perform this labeling effort. What are the main categories of human raters? (check all that apply). 
* Generalists, usually coming from crowdsourcing sites.
* Subject matter experts. A classical example is radiologists labeling medical images for automated diagnosis tools.
* Site-users. Users can provide labels within your application. A classical example is photo tagging.